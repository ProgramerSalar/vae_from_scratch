------------------------------------------- Epoch: [0]
----------------------------------------------------------- Iteration
what is the batch size >>>>>>>>>>>>>>>>>>: torch.Size([1, 3, 16, 256, 256])
what is the shape of logits_fake: >>>>>>>>>>> torch.Size([1, 1, 3, 14, 14])
tensor(5.4148, device='cuda:0', grad_fn=<AddBackward0>) {'train/total_loss': tensor(5.4148, device='cuda:0'), 'train/logvar': tensor(0., device='cuda:0'), 'train/kl_loss': tensor(32.7187, device='cuda:0'), 'train/nll_loss': tensor(6.2449, device='cuda:0'), 'train/rec_loss': tensor(0.5408, device='cuda:0'), 'train/perception_loss': tensor(0.8372, device='cuda:0'), 'train/d_weight': tensor(2.1539, device='cuda:0'), 'train/disc_factor': tensor(1.), 'train/g_loss': tensor(-0.3854, device='cuda:0')}
tensor(1.5086, device='cuda:0', grad_fn=<MulBackward0>) {'train/disc_loss': tensor(1.5086, device='cuda:0'), 'train/logits_real': tensor(0.4901, device='cuda:0'), 'train/logits_fake': tensor(0.3854, device='cuda:0')}
----------------------------------------------------------- Iteration
what is the batch size >>>>>>>>>>>>>>>>>>: torch.Size([1, 3, 16, 256, 256])
what is the shape of logits_fake: >>>>>>>>>>> torch.Size([1, 1, 3, 14, 14])
tensor(4.9783, device='cuda:0', grad_fn=<AddBackward0>) {'train/total_loss': tensor(4.9783, device='cuda:0'), 'train/logvar': tensor(1.0000e-04, device='cuda:0'), 'train/kl_loss': tensor(122.3237, device='cuda:0'), 'train/nll_loss': tensor(4.7336, device='cuda:0'), 'train/rec_loss': tensor(0.3904, device='cuda:0'), 'train/perception_loss': tensor(0.8300, device='cuda:0'), 'train/d_weight': tensor(0.2916, device='cuda:0'), 'train/disc_factor': tensor(1.), 'train/g_loss': tensor(0.8393, device='cuda:0')}
tensor(6.2996, device='cuda:0', grad_fn=<MulBackward0>) {'train/disc_loss': tensor(6.2996, device='cuda:0'), 'train/logits_real': tensor(-2.2701, device='cuda:0'), 'train/logits_fake': tensor(-0.8393, device='cuda:0')}
----------------------------------------------------------- Iteration
what is the batch size >>>>>>>>>>>>>>>>>>: torch.Size([1, 3, 16, 256, 256])
what is the shape of logits_fake: >>>>>>>>>>> torch.Size([1, 1, 3, 14, 14])
tensor(6.3712, device='cuda:0', grad_fn=<AddBackward0>) {'train/total_loss': tensor(6.3712, device='cuda:0'), 'train/logvar': tensor(0.0002, device='cuda:0'), 'train/kl_loss': tensor(149.1765, device='cuda:0'), 'train/nll_loss': tensor(4.8128, device='cuda:0'), 'train/rec_loss': tensor(0.4023, device='cuda:0'), 'train/perception_loss': tensor(0.7904, device='cuda:0'), 'train/d_weight': tensor(0.3430, device='cuda:0'), 'train/disc_factor': tensor(1.), 'train/g_loss': tensor(4.5436, device='cuda:0')}
tensor(3.6095, device='cuda:0', grad_fn=<MulBackward0>) {'train/disc_loss': tensor(3.6095, device='cuda:0'), 'train/logits_real': tensor(4.1094, device='cuda:0'), 'train/logits_fake': tensor(-4.5436, device='cuda:0')}
----------------------------------------------------------- Iteration
what is the batch size >>>>>>>>>>>>>>>>>>: torch.Size([1, 3, 16, 256, 256])
what is the shape of logits_fake: >>>>>>>>>>> torch.Size([1, 1, 3, 14, 14])
tensor(3.7318, device='cuda:0', grad_fn=<AddBackward0>) {'train/total_loss': tensor(3.7318, device='cuda:0'), 'train/logvar': tensor(0.0003, device='cuda:0'), 'train/kl_loss': tensor(937.5568, device='cuda:0'), 'train/nll_loss': tensor(5.0261, device='cuda:0'), 'train/rec_loss': tensor(0.4166, device='cuda:0'), 'train/perception_loss': tensor(0.8612, device='cuda:0'), 'train/d_weight': tensor(0.3912, device='cuda:0'), 'train/disc_factor': tensor(1.), 'train/g_loss': tensor(-3.3087, device='cuda:0')}
tensor(3.0227, device='cuda:0', grad_fn=<MulBackward0>) {'train/disc_loss': tensor(3.0227, device='cuda:0'), 'train/logits_real': tensor(11.4993, device='cuda:0'), 'train/logits_fake': tensor(3.3087, device='cuda:0')}
----------------------------------------------------------- Iteration
what is the batch size >>>>>>>>>>>>>>>>>>: torch.Size([1, 3, 16, 256, 256])
what is the shape of logits_fake: >>>>>>>>>>> torch.Size([1, 1, 3, 14, 14])
tensor(8.0693, device='cuda:0', grad_fn=<AddBackward0>) {'train/total_loss': tensor(8.0693, device='cuda:0'), 'train/logvar': tensor(0.0004, device='cuda:0'), 'train/kl_loss': tensor(1451.7930, device='cuda:0'), 'train/nll_loss': tensor(7.8320, device='cuda:0'), 'train/rec_loss': tensor(0.7116, device='cuda:0'), 'train/perception_loss': tensor(0.7190, device='cuda:0'), 'train/d_weight': tensor(0.1609, device='cuda:0'), 'train/disc_factor': tensor(1.), 'train/g_loss': tensor(1.4742, device='cuda:0')}
tensor(1.4661, device='cuda:0', grad_fn=<MulBackward0>) {'train/disc_loss': tensor(1.4661, device='cuda:0'), 'train/logits_real': tensor(9.2971, device='cuda:0'), 'train/logits_fake': tensor(-1.4742, device='cuda:0')}
----------------------------------------------------------- Iteration
what is the batch size >>>>>>>>>>>>>>>>>>: torch.Size([1, 3, 16, 256, 256])
what is the shape of logits_fake: >>>>>>>>>>> torch.Size([1, 1, 3, 14, 14])
tensor(3.1229, device='cuda:0', grad_fn=<AddBackward0>) {'train/total_loss': tensor(3.1229, device='cuda:0'), 'train/logvar': tensor(0.0005, device='cuda:0'), 'train/kl_loss': tensor(1885.6104, device='cuda:0'), 'train/nll_loss': tensor(3.0010, device='cuda:0'), 'train/rec_loss': tensor(0.2270, device='cuda:0'), 'train/perception_loss': tensor(0.7324, device='cuda:0'), 'train/d_weight': tensor(0.0423, device='cuda:0'), 'train/disc_factor': tensor(1.), 'train/g_loss': tensor(2.8803, device='cuda:0')}
tensor(1.4073, device='cuda:0', grad_fn=<MulBackward0>) {'train/disc_loss': tensor(1.4073, device='cuda:0'), 'train/logits_real': tensor(0.7104, device='cuda:0'), 'train/logits_fake': tensor(-2.8803, device='cuda:0')}
----------------------------------------------------------- Iteration
what is the batch size >>>>>>>>>>>>>>>>>>: torch.Size([1, 3, 16, 256, 256])
what is the shape of logits_fake: >>>>>>>>>>> torch.Size([1, 1, 3, 14, 14])
tensor(4.4789, device='cuda:0', grad_fn=<AddBackward0>) {'train/total_loss': tensor(4.4789, device='cuda:0'), 'train/logvar': tensor(0.0006, device='cuda:0'), 'train/kl_loss': tensor(2057.7075, device='cuda:0'), 'train/nll_loss': tensor(4.4598, device='cuda:0'), 'train/rec_loss': tensor(0.3672, device='cuda:0'), 'train/perception_loss': tensor(0.7897, device='cuda:0'), 'train/d_weight': tensor(0.0245, device='cuda:0'), 'train/disc_factor': tensor(1.), 'train/g_loss': tensor(0.7818, device='cuda:0')}
tensor(0.9836, device='cuda:0', grad_fn=<MulBackward0>) {'train/disc_loss': tensor(0.9836, device='cuda:0'), 'train/logits_real': tensor(3.0330, device='cuda:0'), 'train/logits_fake': tensor(-0.7818, device='cuda:0')}
----------------------------------------------------------- Iteration
what is the batch size >>>>>>>>>>>>>>>>>>: torch.Size([1, 3, 16, 256, 256])
what is the shape of logits_fake: >>>>>>>>>>> torch.Size([1, 1, 3, 14, 14])
tensor(4.9011, device='cuda:0', grad_fn=<AddBackward0>) {'train/total_loss': tensor(4.9011, device='cuda:0'), 'train/logvar': tensor(0.0007, device='cuda:0'), 'train/kl_loss': tensor(2112.3057, device='cuda:0'), 'train/nll_loss': tensor(4.0428, device='cuda:0'), 'train/rec_loss': tensor(0.3245, device='cuda:0'), 'train/perception_loss': tensor(0.7999, device='cuda:0'), 'train/d_weight': tensor(0.3056, device='cuda:0'), 'train/disc_factor': tensor(1.), 'train/g_loss': tensor(2.8091, device='cuda:0')}
tensor(0.6795, device='cuda:0', grad_fn=<MulBackward0>) {'train/disc_loss': tensor(0.6795, device='cuda:0'), 'train/logits_real': tensor(3.6870, device='cuda:0'), 'train/logits_fake': tensor(-2.8091, device='cuda:0')}
----------------------------------------------------------- Iteration
what is the batch size >>>>>>>>>>>>>>>>>>: torch.Size([1, 3, 16, 256, 256])
what is the shape of logits_fake: >>>>>>>>>>> torch.Size([1, 1, 3, 14, 14])
tensor(4.2515, device='cuda:0', grad_fn=<AddBackward0>) {'train/total_loss': tensor(4.2515, device='cuda:0'), 'train/logvar': tensor(0.0008, device='cuda:0'), 'train/kl_loss': tensor(2290.6311, device='cuda:0'), 'train/nll_loss': tensor(3.4695, device='cuda:0'), 'train/rec_loss': tensor(0.2735, device='cuda:0'), 'train/perception_loss': tensor(0.7360, device='cuda:0'), 'train/d_weight': tensor(0.3018, device='cuda:0'), 'train/disc_factor': tensor(1.), 'train/g_loss': tensor(2.5914, device='cuda:0')}
tensor(0.4201, device='cuda:0', grad_fn=<MulBackward0>) {'train/disc_loss': tensor(0.4201, device='cuda:0'), 'train/logits_real': tensor(5.4280, device='cuda:0'), 'train/logits_fake': tensor(-2.5914, device='cuda:0')}
----------------------------------------------------------- Iteration
what is the batch size >>>>>>>>>>>>>>>>>>: torch.Size([1, 3, 16, 256, 256])
what is the shape of logits_fake: >>>>>>>>>>> torch.Size([1, 1, 3, 14, 14])
tensor(5.2822, device='cuda:0', grad_fn=<AddBackward0>) {'train/total_loss': tensor(5.2822, device='cuda:0'), 'train/logvar': tensor(0.0009, device='cuda:0'), 'train/kl_loss': tensor(2657.8484, device='cuda:0'), 'train/nll_loss': tensor(3.8446, device='cuda:0'), 'train/rec_loss': tensor(0.3073, device='cuda:0'), 'train/perception_loss': tensor(0.7742, device='cuda:0'), 'train/d_weight': tensor(0.3750, device='cuda:0'), 'train/disc_factor': tensor(1.), 'train/g_loss': tensor(3.8339, device='cuda:0')}
tensor(1.2793, device='cuda:0', grad_fn=<MulBackward0>) {'train/disc_loss': tensor(1.2793, device='cuda:0'), 'train/logits_real': tensor(-1.3861, device='cuda:0'), 'train/logits_fake': tensor(-3.8339, device='cuda:0')}
----------------------------------------------------------- Iteration
what is the batch size >>>>>>>>>>>>>>>>>>: torch.Size([1, 3, 16, 256, 256])
what is the shape of logits_fake: >>>>>>>>>>> torch.Size([1, 1, 3, 14, 14])
tensor(5.3964, device='cuda:0', grad_fn=<AddBackward0>) {'train/total_loss': tensor(5.3964, device='cuda:0'), 'train/logvar': tensor(0.0009, device='cuda:0'), 'train/kl_loss': tensor(2410.8027, device='cuda:0'), 'train/nll_loss': tensor(5.2654, device='cuda:0'), 'train/rec_loss': tensor(0.4450, device='cuda:0'), 'train/perception_loss': tensor(0.8196, device='cuda:0'), 'train/d_weight': tensor(0.1301, device='cuda:0'), 'train/disc_factor': tensor(1.), 'train/g_loss': tensor(1.0067, device='cuda:0')}