{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c9e4c4",
   "metadata": {},
   "source": [
    "# 1. dummy time_embedding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccad48e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0657, -0.2787, -1.2110,  ...,  0.5981, -0.5768, -0.5393],\n",
       "        [ 1.1917, -0.1065,  1.9694,  ..., -0.1435, -1.1355, -0.3291]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "# (batch_size, tembedding_channels)\n",
    "temb = torch.randn(2, 512)\n",
    "temb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f46e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 1, 1, 1, 1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1,) * 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756c5f08",
   "metadata": {},
   "source": [
    "# 2. AdaGroupNorm\n",
    "\n",
    "`for - images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bf18cb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaGroupNorm(\n",
      "  (linear): Linear(in_features=6, out_features=12, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 6, 64, 64])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers.models.normalization import AdaGroupNorm\n",
    "import torch \n",
    "\n",
    "# in_channels = 3\n",
    "out_channels = 6\n",
    "temb_channels = 6\n",
    "\n",
    "x = torch.randn(2, out_channels,  64, 64)\n",
    "temb = torch.randn(2, temb_channels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ada_group_normalization = AdaGroupNorm(embedding_dim=temb_channels,   # make sure in `embedding_dim` to put the `time_embedding` that is 256 in here.\n",
    "                                       out_dim=out_channels,\n",
    "                                       num_groups=2,\n",
    "                                       eps=1e-6)\n",
    "\n",
    "print(ada_group_normalization)\n",
    "\n",
    "ada_group_normalization = ada_group_normalization(x, temb)\n",
    "ada_group_normalization.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f5889f",
   "metadata": {},
   "source": [
    "`for - video`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01205bea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaGroupNorm(\n",
      "  (linear): Linear(in_features=8, out_features=128, bias=True)\n",
      ")\n",
      "what is the shape of row data: torch.Size([16, 64, 64, 64]) and what is the shape of scale: torch.Size([16, 64, 1, 1]) and what is the shape of shift: torch.Size([16, 64, 1, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 64, 64, 64])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers.models.normalization import AdaGroupNorm\n",
    "import torch \n",
    "\n",
    "in_channels = 3\n",
    "out_channels = 64\n",
    "temb_channels = 8\n",
    "batch_size = 2\n",
    "frame = 8 \n",
    "\n",
    "batch_frame = batch_size * frame\n",
    "\n",
    "# video_x = torch.randn(batch_size, out_channels, frame,  64, 64)\n",
    "image_x = torch.randn(batch_frame, out_channels, 64, 64) # so the total image = 2 * 8 => 16 images\n",
    "temb = torch.randn(batch_frame, temb_channels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ada_group_normalization = AdaGroupNorm(embedding_dim=temb_channels,   # make sure in `embedding_dim` to put the `time_embedding` that is 256 in here.\n",
    "                                       out_dim=out_channels,\n",
    "                                       num_groups=batch_size,\n",
    "                                       eps=1e-6)\n",
    "\n",
    "print(ada_group_normalization)\n",
    "\n",
    "ada_group_normalization = ada_group_normalization(image_x, temb)\n",
    "ada_group_normalization.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6d1a1",
   "metadata": {},
   "source": [
    "# 3. SpatialNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f673eb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "from torch.nn import functional as F \n",
    "\n",
    "class SpatialNorm(nn.Module):\n",
    "    \"\"\"\n",
    "    Spatially conditioned normalization as defined in https://arxiv.org/abs/2209.09002.\n",
    "\n",
    "    Args:\n",
    "        f_channels (`int`):\n",
    "            The number of channels for input to group normalization layer, and output of the spatial norm layer.\n",
    "        zq_channels (`int`):\n",
    "            The number of channels for the quantized vector as described in the paper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        f_channels: int,\n",
    "        zq_channels: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm_layer = nn.GroupNorm(num_channels=f_channels, num_groups=32, eps=1e-6, affine=True)\n",
    "        self.conv_y = nn.Conv2d(zq_channels, f_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.conv_b = nn.Conv2d(zq_channels, f_channels, kernel_size=1, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, f: torch.Tensor, zq: torch.Tensor) -> torch.Tensor:\n",
    "        f_size = f.shape[-2:] \n",
    "        zq = F.interpolate(zq, size=f_size, mode=\"nearest\")\n",
    "        norm_f = self.norm_layer(f)\n",
    "        new_f = norm_f * self.conv_y(zq) + self.conv_b(zq)\n",
    "        return new_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "021a094f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpatialNorm(\n",
      "  (norm_layer): GroupNorm(32, 128, eps=1e-06, affine=True)\n",
      "  (conv_y): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv_b): Conv2d(16, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "what is the shape of f : torch.Size([2, 128, 64, 64]) and zq: torch.Size([2, 16, 8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 128, 64, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "num_channels = 64\n",
    "zq_channels = 8\n",
    "\n",
    "spatial_norm = SpatialNorm(f_channels=128,\n",
    "                           zq_channels=16)\n",
    "\n",
    "print(spatial_norm)\n",
    "\n",
    "# feature map \n",
    "x = torch.randn(2, 128, 64, 64)\n",
    "# quantized vector \n",
    "z = torch.randn(2, 16, 8, 8)\n",
    "\n",
    "spatial_norm = spatial_norm(x, z)\n",
    "spatial_norm.shape\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280eea31",
   "metadata": {},
   "source": [
    "# 4. torch.interpolate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70a0bfe",
   "metadata": {},
   "source": [
    "#### `mode: 'nearest'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "58fe52a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 1024, 1024])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# x = torch.rand(1, 1, 2, 2)\n",
    "x = torch.rand(32, 3, 256, 256)\n",
    "\n",
    "nearest_interpolate = F.interpolate(x, \n",
    "                                #  size=(64, 64),\n",
    "                                scale_factor=(4, 4),\n",
    "                                 mode='nearest')\n",
    "nearest_interpolate.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea65704c",
   "metadata": {},
   "source": [
    "#### `mode: 'Linear'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "35fafaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 8])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size = 32, channels=3, length=4\n",
    "x_linear = torch.randn(32, 3, 4)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "linear_interpolate = F.interpolate(x_linear,\n",
    "                                   size=8,\n",
    "                                   mode='linear',\n",
    "                                   align_corners=False  # a/c to pytorch library\n",
    "                                   )\n",
    "\n",
    "linear_interpolate.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a319f7",
   "metadata": {},
   "source": [
    "#### `mode: 'BiLinear'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "67bac6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 64, 64])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "bilinear_interpolate = F.interpolate(x,\n",
    "                                     size=(64, 64),\n",
    "                                     mode='bilinear')\n",
    "bilinear_interpolate.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea34d7",
   "metadata": {},
   "source": [
    "# 5. conv 3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa48f8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural_Network(\n",
      "  (norm1): GroupNorm(2, 16, eps=1e-06, affine=True)\n",
      "  (conv1): Conv3d(16, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1))\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 8, 256, 256])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "from torch import nn \n",
    "from typing import Optional, Tuple, Union\n",
    "\n",
    "class Neural_Network(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 out_channels: int,\n",
    "                 kernel_size: Union[int, Tuple[int, int, int]] = 3,\n",
    "                 stride: Union[int, Tuple[int, int, int]] = 1,\n",
    "                 group: Optional[int] = 32,\n",
    "                 dilation: Optional[int] = 1,\n",
    "                 padding: Optional[int] = 1):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = nn.GroupNorm(num_groups=group,\n",
    "                             num_channels=in_channels,\n",
    "                             eps=1e-6)\n",
    "        \n",
    "        self.conv1 = nn.Conv3d(in_channels=in_channels,\n",
    "                          out_channels=out_channels,\n",
    "                          kernel_size=kernel_size,\n",
    "                          stride=stride,\n",
    "                          padding=padding,\n",
    "                          dilation=dilation)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.norm1(x)\n",
    "        x = self.conv1(x)\n",
    "        return x \n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "x = torch.randn(2, 16, 8, 256, 256)\n",
    "\n",
    "neural_network = Neural_Network(in_channels=16,\n",
    "                                out_channels=16,\n",
    "                                group=2)\n",
    "\n",
    "print(neural_network)\n",
    "\n",
    "output = neural_network(x)\n",
    "output.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f319d5cb",
   "metadata": {},
   "source": [
    "# 6. if add the two different dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2adb5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 32, 32])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.randn(2, 16, 32, 32)\n",
    "temb = torch.rand(2, 16)\n",
    "\n",
    "temb = temb[:, :, None, None]\n",
    "\n",
    "\n",
    "\n",
    "add = x + temb\n",
    "add.shape  # [2, 16, 32, 32]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c775e05",
   "metadata": {},
   "source": [
    "# 7. torch.chunk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f21e944a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 128]), torch.Size([2, 128]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "temb = torch.randn(2, 256)\n",
    "\n",
    "a, b = torch.chunk(temb,\n",
    "                         chunks=2,\n",
    "                         dim=1)\n",
    "\n",
    "a.shape, b.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e1fb373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 8, 68, 68])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.randn(2, 32, 8, 68, 68)\n",
    "\n",
    "conv = torch.nn.Conv3d(in_channels=32,\n",
    "                       out_channels=64,\n",
    "                       kernel_size=3,\n",
    "                       stride=1,\n",
    "                       padding=1,\n",
    "                        dilation=1,\n",
    "                        groups=2)\n",
    "\n",
    "conv = conv(x)\n",
    "conv.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf3e3e8",
   "metadata": {},
   "source": [
    "# 8. what is the meaning Pixelshuffle() and PixelUnshuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa253ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 512, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "x = torch.randn(2, 4, 256, 256) # output = [2, 1, 512, 512]\n",
    "x = torch.randn(2, 8, 256, 256) # output = [2, 2, 512, 512]\n",
    "\n",
    "# make sure the `input_channels` is square of `upscale_factor` \n",
    "# out_channels = in_channels / upscaling^2, height, width * upscaling  \n",
    "pixel_shuffle = torch.nn.PixelShuffle(2) # upscale the channels \n",
    "output = pixel_shuffle(x)\n",
    "output.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52d3860",
   "metadata": {},
   "source": [
    "in_channels = 4 \n",
    "upscaling = 2*2 = 4 \n",
    "\n",
    "out_channels = in_channels / upscaling\n",
    "out_channels = 4 / 4 = 1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b83a31c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 16, 128, 128])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "x = torch.randn(2, 4, 256, 256)\n",
    "\n",
    "# out_channels = in_channels * downscale^2, height, width / downscale \n",
    "pixel_unshuffle = torch.nn.PixelUnshuffle(2)\n",
    "output = pixel_unshuffle(x)\n",
    "output.shape # [2, 1, 128, 128]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6060e2c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_output = (64,)\n",
    "block_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129a6dd4",
   "metadata": {},
   "source": [
    "# 9. torch.layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43a4c991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tensor = torch.randn(2, 3)\n",
    "tensor.layout\n",
    "\n",
    "converted_shape = tensor.t().stride()\n",
    "converted_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e028e",
   "metadata": {},
   "source": [
    "# 10. torch.generator()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99156774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8784880626046803224"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "generator = torch.Generator(device=device)\n",
    "generator.seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fadbedf",
   "metadata": {},
   "source": [
    "# 11. diffusers.register_to_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "831fb0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.configuration_utils import register_to_config, ConfigMixin\n",
    "\n",
    "class vae_Config(ConfigMixin):\n",
    "\n",
    "    @register_to_config\n",
    "    def __init__(self,\n",
    "                 model_name=\"bert\",\n",
    "                 hidden_size=768,\n",
    "                 num_layers=12,\n",
    "                 temporary_param=None,\n",
    "                 _private_param=None):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.temporary_param = temporary_param\n",
    "        self._private_param = _private_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4482b70e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "vae_Config {\n",
       "  \"_class_name\": \"vae_Config\",\n",
       "  \"_diffusers_version\": \"0.33.1\",\n",
       "  \"_private_param\": null,\n",
       "  \"hidden_size\": 768,\n",
       "  \"model_name\": \"bert\",\n",
       "  \"num_layers\": 12,\n",
       "  \"temporary_param\": null\n",
       "}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae_Config.config_name = \"vae_Config\"\n",
    "model_config = vae_Config()\n",
    "model_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9352873c",
   "metadata": {},
   "source": [
    "## 12. nn.register_buffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d218b8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 1, 1])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "class Test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.register_buffer('tensor1', torch.tensor([0.2, 0.3, 0.4])[None, :, None, None])\n",
    "        \n",
    "\n",
    "    def forward(self):\n",
    "        return self.tensor2\n",
    "    \n",
    "\n",
    "a = Test()\n",
    "a = a()\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa305ef",
   "metadata": {},
   "source": [
    "## 13. torchvision.models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac4066a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (6): ReLU(inplace=True)\n",
       "  (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (8): ReLU(inplace=True)\n",
       "  (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (15): ReLU(inplace=True)\n",
       "  (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (18): ReLU(inplace=True)\n",
       "  (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (20): ReLU(inplace=True)\n",
       "  (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (22): ReLU(inplace=True)\n",
       "  (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (25): ReLU(inplace=True)\n",
       "  (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (27): ReLU(inplace=True)\n",
       "  (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (29): ReLU(inplace=True)\n",
       "  (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchvision import models\n",
    "\n",
    "vgg_16 = models.vgg16().features\n",
    "vgg_16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1583567e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 256, 256])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn \n",
    "from collections import namedtuple\n",
    "from torchvision import models\n",
    "\n",
    "\n",
    "class Vgg16(nn.Module):\n",
    "    def __init__(self,\n",
    "                 requires_grad=False,\n",
    "                 pretrained=True):\n",
    "        super().__init__()\n",
    "        \n",
    "        vgg_pretrained_features = models.vgg16().features\n",
    "        self.slice1 = nn.Sequential()\n",
    "        self.slice2 = nn.Sequential()\n",
    "        self.slice3 = nn.Sequential()\n",
    "        self.slice4 = nn.Sequential()\n",
    "        self.slice5 = nn.Sequential()\n",
    "        \n",
    "        \n",
    "        for i in range(4): \n",
    "            self.slice1.add_module(name=str(i), module=vgg_pretrained_features[i]) \n",
    "        for i in range(4, 9): \n",
    "            self.slice2.add_module(name=str(i), module=vgg_pretrained_features[i])\n",
    "        for i in range(9, 16):  \n",
    "            self.slice3.add_module(name=str(i), module=vgg_pretrained_features[i])\n",
    "        for i in range(16, 23):  \n",
    "            self.slice4.add_module(name=str(i), module=vgg_pretrained_features[i])\n",
    "        for i in range(23, 30):  \n",
    "            self.slice5.add_module(name=str(i), module=vgg_pretrained_features[i])\n",
    "\n",
    "        if not requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h1 = self.slice1(x)\n",
    "        h2 = self.slice2(h1)\n",
    "        h3 = self.slice3(h2)\n",
    "        h4 = self.slice4(h3)\n",
    "        h5 = self.slice5(h4)\n",
    "\n",
    "        vgg_outputs = namedtuple(typename=\"VggOutputs\", field_names=['h1', 'h2', 'h3', 'h4', 'h5'])\n",
    "        out = vgg_outputs(h1, h2, h3, h4, h5)\n",
    "\n",
    "        return out \n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "out = Vgg16()\n",
    "x = torch.randn(2, 3, 256, 256)\n",
    "out = out(x)\n",
    "out.h1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c689de31",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetLinLayer(nn.Module):\n",
    "    \"\"\"A single linear layer which does a 1x1 conv.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 out_channels: int = 1,\n",
    "                 use_dropout: bool = False):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential([\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(in_channels=in_channels,\n",
    "                      out_channels=out_channels,\n",
    "                      kernel_size=1,\n",
    "                      stride=1,\n",
    "                      padding=0,\n",
    "                      bias=False)\n",
    "        ])\n",
    "\n",
    "out = NetLinLayer(in_channels=3)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "760fe177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n",
      "tensor([1.])\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.]])\n",
      "tensor([[[1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[1., 1.],\n",
      "         [1., 1.]]])\n",
      "tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "        [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "         [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          ...,\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "          [1., 1., 1.,  ..., 1., 1., 1.]]]])\n",
      "tensor([[[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "         [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "         [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "         [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]]],\n",
      "\n",
      "\n",
      "         [[[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
      "\n",
      "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           ...,\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
      "           [1., 1., 1.,  ..., 1., 1., 1.]]]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.ones(())) # zero dim \n",
    "print(torch.ones(1)) # one dim \n",
    "print(torch.ones(2, 2)) # two dim \n",
    "print(torch.ones(2, 2, 2))  # three dim\n",
    "print(torch.ones(2, 3, 128, 128))   # four dim\n",
    "print(torch.ones(2, 3, 4, 128, 128))   # five dim\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a0c667",
   "metadata": {},
   "source": [
    "## 15. init distributed mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5e3b87",
   "metadata": {},
   "source": [
    "Of course. This Python code is designed to set up a distributed computing environment, specifically for situations where the script is launched using Open MPI (a common framework for parallel computing). It acts as a compatibility layer, translating Open MPI's environment variables into a format that a different library, likely PyTorch Distributed, expects.\n",
    "\n",
    "However, there appears to be a **critical bug** in the code you provided.\n",
    "\n",
    "-----\n",
    "\n",
    "### \\#\\# Code Explanation\n",
    "\n",
    "The primary goal is to detect if the program is running in a multi-process, multi-machine setup managed by Open MPI and then configure it accordingly.\n",
    "\n",
    "1.  **Detecting the Open MPI Environment**\n",
    "\n",
    "    ```python\n",
    "    if int(os.getenv('OMPI_COMM_WORLD_SIZE', '0')) > 0:\n",
    "    ```\n",
    "\n",
    "      * This line checks for an environment variable named `OMPI_COMM_WORLD_SIZE`.\n",
    "      * When you launch a program with Open MPI's `mpirun`, it automatically sets this variable to the total number of processes you requested. For example, `mpirun -np 8 ...` would set it to `'8'`.\n",
    "      * `os.getenv(..., '0')` safely gets the value, defaulting to `'0'` if it doesn't exist.\n",
    "      * The `if` condition is true only if the script is launched by Open MPI with more than one process.\n",
    "\n",
    "2.  **Reading Open MPI Variables**\n",
    "\n",
    "    ```python\n",
    "    rank = int(os.environ['OMPI_COMM_WORLD_RANK'])\n",
    "    local_rank = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n",
    "    world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])\n",
    "    ```\n",
    "\n",
    "    If the code is in an Open MPI environment, it reads these key variables:\n",
    "\n",
    "      * `OMPI_COMM_WORLD_SIZE`: The **world size**, or the total number of processes in the job.\n",
    "      * `OMPI_COMM_WORLD_RANK`: The **global rank**, a unique ID for the current process, from `0` to `world_size - 1`.\n",
    "      * `OMPI_COMM_WORLD_LOCAL_RANK`: The **local rank**, which is the process's unique ID *on its specific machine*. This is crucial for assigning one GPU per process on a multi-GPU server.\n",
    "\n",
    "3.  **Mapping to Standard Variables (with a Bug)**\n",
    "\n",
    "    ```python\n",
    "    # This block contains an error\n",
    "    os.environ[\"LOCAL_RANK\"] = os.environ['OMPI_COMM_WORLD_LOCAL_RANK']\n",
    "    os.environ[\"RANK\"] = os.environ['OMPI_COMM_WORLD_SIZE']\n",
    "    os.environ[\"WORLD_SIZE\"] = os.environ['OMPI_COMM_WORLD_RANK']\n",
    "    ```\n",
    "\n",
    "    This section tries to create the standard environment variables (`LOCAL_RANK`, `RANK`, `WORLD_SIZE`) that frameworks like PyTorch Distributed use for auto-configuration.\n",
    "\n",
    "      * `LOCAL_RANK` is set correctly.\n",
    "      * **BUG:** `RANK` and `WORLD_SIZE` are swapped. The global rank is being set to the world size, and the world size is being set to the global rank. This will cause the distributed setup to fail.\n",
    "\n",
    "4.  **Setting Program Arguments**\n",
    "\n",
    "    ```python\n",
    "    args.rank = int(os.environ[\"RANK\"])\n",
    "    args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    args.gpu = int(os.environ[\"LOCAL_RANK\"])\n",
    "    ```\n",
    "\n",
    "    Finally, it populates an `args` object (likely from `argparse`) with these values so the rest of the application can easily use them, for instance, to assign the correct GPU to the process.\n",
    "\n",
    "-----\n",
    "\n",
    "### \\#\\# The Corrected Code\n",
    "\n",
    "To fix the bug, you need to swap the assignments for `RANK` and `WORLD_SIZE`.\n",
    "\n",
    "```python\n",
    "# Corrected version\n",
    "if int(os.getenv('OMPI_COMM_WORLD_SIZE', '0')) > 0:\n",
    "\n",
    "    # Set the standard variables correctly\n",
    "    os.environ[\"LOCAL_RANK\"] = os.environ['OMPI_COMM_WORLD_LOCAL_RANK']\n",
    "    os.environ[\"RANK\"] = os.environ['OMPI_COMM_WORLD_RANK']          # Corrected\n",
    "    os.environ[\"WORLD_SIZE\"] = os.environ['OMPI_COMM_WORLD_SIZE']      # Corrected\n",
    "\n",
    "    # Populate the args object from the newly set variables\n",
    "    args.rank = int(os.environ[\"RANK\"])\n",
    "    args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    args.gpu = int(os.environ[\"LOCAL_RANK\"])\n",
    "```\n",
    "\n",
    "In short, this code is a bridge that allows a program expecting PyTorch's distributed environment to be launched using Open MPI's `mpirun` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ce5da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import argparse as args\n",
    "\n",
    "if int(os.getenv('OMPI_COMM_WORLD_SIZE', '0')) > 0:\n",
    "    rank = int(os.environ['OMPI_COMM_WORLD_RANK'])\n",
    "    local_rank = int(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\n",
    "    world_size = int(os.environ['OMPI_COMM_WORLD_SIZE'])\n",
    "\n",
    "    os.environ[\"LOCAL_RANK\"] = os.environ['OMPI_COMM_WORLD_LOCAL_RANK']\n",
    "    os.environ[\"RANK\"] = os.environ['OMPI_COMM_WORLD_RANK']\n",
    "    os.environ[\"WORLD_SIZE\"] = os.environ['OMPI_COMM_WORLD_SIZE']\n",
    "\n",
    "    args.rank = int(os.environ[\"RANK\"])\n",
    "    args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
    "    args.gpu = int(os.environ[\"LOCAL_RANK\"])\n",
    "\n",
    "elif 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "    args.rank = int(os.environ[\"RANK\"])\n",
    "    args.world_size = int(os.environ['WORLD_SIZE'])\n",
    "    args.gpu = int(os.environ['LOCAL_RANK'])\n",
    "\n",
    "else:\n",
    "    print('Not using distributed mode')\n",
    "    args.distributed = False\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8819483f",
   "metadata": {},
   "source": [
    "This code configures the settings for a distributed training session, most commonly used in a framework like PyTorch. Each line sets a specific parameter for how multiple processes (running on different GPUs or machines) should coordinate.\n",
    "\n",
    "---\n",
    "\n",
    "### ## `args.distributed = True`\n",
    "\n",
    "This is a simple boolean flag that **enables distributed mode**. 🚩\n",
    "\n",
    "Your training script will likely have `if` statements that check this flag. If it's `True`, the script will execute code to set up communication between processes. If `False`, it will run as a standard, single-process program.\n",
    "\n",
    "---\n",
    "\n",
    "### ## `args.dist_backend = 'nccl'`\n",
    "\n",
    "This line sets the **communication backend** to `'nccl'`.\n",
    "\n",
    "The backend is the underlying library that manages how data (like gradients and model weights) is sent between your GPUs.\n",
    "\n",
    "* **`nccl`** stands for the **NVIDIA Collective Communications Library**. It is a highly optimized library for communication between NVIDIA GPUs. It's the standard and fastest choice for multi-GPU training on NVIDIA hardware. Other options exist, like `gloo` (for CPUs) or `mpi`, but `nccl` is preferred for deep learning with CUDA.\n",
    "\n",
    "---\n",
    "\n",
    "### ## `args.dist_url = \"env://\"`\n",
    "\n",
    "This line tells the program **how to find the other processes** to form a communication group.\n",
    "\n",
    "The `\"env://\"` method instructs the program to initialize itself by reading connection details from the **environment variables**. This is the most common and flexible approach because a launch utility like PyTorch's `torchrun` will automatically set the required variables (`MASTER_ADDR`, `MASTER_PORT`, `RANK`, and `WORLD_SIZE`) for each process it starts. This way, you don't have to hardcode IP addresses or other machine-specific details into your script.\n",
    "\n",
    "In summary, these three lines work together to tell your script: \"Yes, run in **distributed mode** using the fast **NCCL backend** for GPUs, and get the connection details from the **environment variables** set up by the launcher.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5feaf015",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.distributed = True\n",
    "args.dist_backend = 'nccl'\n",
    "args.dist_url = \"env://\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004427d3",
   "metadata": {},
   "source": [
    "This code block initializes the PyTorch Distributed Data Parallel (DDP) environment, which is the standard way to perform multi-GPU or multi-machine training in PyTorch. It ensures that all the separate processes can communicate and synchronize with each other.\n",
    "\n",
    "-----\n",
    "\n",
    "### \\#\\# Conditional Initialization\n",
    "\n",
    "```python\n",
    "if init_pytorch_ddp:\n",
    "```\n",
    "\n",
    "This is a simple check. The code inside this `if` block only runs if the `init_pytorch_ddp` variable is `True`. This is a useful way to control whether the script should handle the DDP setup itself or if it's being managed by an external framework like Hugging Face's `accelerate` (as the comment suggests).\n",
    "\n",
    "-----\n",
    "\n",
    "### \\#\\# Assigning a GPU\n",
    "\n",
    "```python\n",
    "torch.cuda.set_device(args.gpu)\n",
    "```\n",
    "\n",
    "This line is crucial for multi-GPU setups. It **binds the current process to a specific GPU**. 📌\n",
    "\n",
    "In distributed training, you launch one process for each GPU. This command uses the `args.gpu` value (which is typically the process's **local rank**) to ensure that each process exclusively controls one GPU. For example, the process with `local_rank=0` takes control of GPU 0, `local_rank=1` takes GPU 1, and so on. This prevents conflicts and ensures memory is managed correctly.\n",
    "\n",
    "-----\n",
    "\n",
    "### \\#\\# Initializing the Process Group\n",
    "\n",
    "```python\n",
    "torch.distributed.init_process_group(backend=args.dist_backend,\n",
    "                                     init_method=args.dist_url,\n",
    "                                     world_size=args.world_size,\n",
    "                                     rank=args.rank,\n",
    "                                     timeout=timedelta(days=365))\n",
    "```\n",
    "\n",
    "This is the main command that **establishes the connection between all processes**. 🤝 It's like the official start of a conference call where everyone joins and can now hear each other.\n",
    "\n",
    "  * `backend=args.dist_backend`: Sets the communication library, which is typically `'nccl'` for fast communication between NVIDIA GPUs.\n",
    "  * `init_method=args.dist_url`: Tells the processes how to find each other. As discussed, this is usually `\"env://\"` so the processes can read the connection info from environment variables.\n",
    "  * `world_size=args.world_size`: Informs this process about the total number of processes participating in the training job.\n",
    "  * `rank=args.rank`: Gives this specific process its unique global ID.\n",
    "  * `timeout=timedelta(days=365)`: This sets an extremely long timeout for operations. If one process has to wait for another for longer than this period, it will raise an error. Setting a long timeout can prevent your job from crashing during long data loading phases or when debugging.\n",
    "\n",
    "-----\n",
    "\n",
    "### \\#\\# Synchronizing All Processes\n",
    "\n",
    "```python\n",
    "torch.distributed.barrier()\n",
    "```\n",
    "\n",
    "This command acts as a **synchronization point**. 🚧\n",
    "\n",
    "When a process executes `barrier()`, it will pause and wait until **every single process** in the group has also reached this exact point. This ensures that no process moves on to the next step (like loading the model) until all other processes have successfully finished the initialization. It prevents race conditions and guarantees everyone starts on the same page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a15d59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "init_pytorch_ddp = True\n",
    "\n",
    "if init_pytorch_ddp:\n",
    "    # Init DDP Group, for script without using accelerate framework\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    torch.distributed.init_process_group(backend=args.dist_backend, \n",
    "                                         init_method=args.dist_url,\n",
    "                                        world_size=args.world_size, \n",
    "                                        rank=args.rank, \n",
    "                                        timeout=timedelta(days=365))\n",
    "    torch.distributed.barrier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78d0ce8",
   "metadata": {},
   "source": [
    "This function cleverly modifies Python's built-in `print()` function to control which process can write output in a distributed computing environment.\n",
    "\n",
    "Its main purpose is to **prevent log spam**. In distributed training, you run the same script on multiple GPUs or machines simultaneously. If every process prints logs, your console output becomes a messy and unreadable wall of duplicated text. This function ensures that, by default, **only the main process (the \"master\") can print messages**.\n",
    "\n",
    "-----\n",
    "\n",
    "### \\#\\# How It Works\n",
    "\n",
    "1.  **Saves the Original `print`**:\n",
    "\n",
    "    ```python\n",
    "    import builtins as __builtin__\n",
    "    builtin_print = __builtin__.print\n",
    "    ```\n",
    "\n",
    "    First, it gets a reference to the original, built-in `print` function and saves it in a variable called `builtin_print`. This is essential because it needs a way to call the real `print` function later.\n",
    "\n",
    "2.  **Creates a New, Custom `print`**:\n",
    "\n",
    "    ```python\n",
    "    def print(*args, **kwargs):\n",
    "        force = kwargs.pop('force', False)\n",
    "        if is_master or force:\n",
    "            builtin_print(*args, **kwargs)\n",
    "    ```\n",
    "\n",
    "    Next, it defines a new function, which is also named `print`. This new function contains the special logic:\n",
    "\n",
    "      * It checks for a special keyword argument called `force`. If you call `print(\"message\", force=True)`, it will set `force` to `True`. Otherwise, `force` is `False`.\n",
    "      * The `if is_master or force:` condition is the core of the function. It allows printing only if one of two things is true:\n",
    "          * The process is the master process (`is_master` is `True`).\n",
    "          * The print call was explicitly told to override the rule (`force` is `True`).\n",
    "      * If the condition passes, it calls the original `builtin_print` to display the message. Otherwise, it does nothing, effectively silencing the print statement.\n",
    "\n",
    "3.  **Replaces the Built-in `print`**:\n",
    "\n",
    "    ```python\n",
    "    __builtin__.print = print\n",
    "    ```\n",
    "\n",
    "    Finally, this line overwrites the global, built-in `print` with the new custom `print` function. This technique is called **monkey-patching**. From this point on, whenever any part of your code calls `print()`, it will execute your new, conditional version instead of the original one.\n",
    "\n",
    "-----\n",
    "\n",
    "### \\#\\# Practical Example\n",
    "\n",
    "Imagine you call this function at the start of your script:\n",
    "\n",
    "```python\n",
    "# In the master process (rank 0)\n",
    "is_master = True\n",
    "setup_for_distributed(is_master)\n",
    "\n",
    "# In a worker process (rank > 0)\n",
    "is_master = False\n",
    "setup_for_distributed(is_master)\n",
    "\n",
    "# --- Now, in both processes ---\n",
    "print(\"Epoch 1 starting...\")  # Only prints on the master process\n",
    "print(\"Debugging worker.\", force=True) # Prints on ALL processes because of 'force'\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98d7b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_for_distributed(is_master):\n",
    "    \"\"\"\n",
    "    This function disables printing when not in master process\n",
    "    \"\"\"\n",
    "    import builtins as __builtin__\n",
    "    builtin_print = __builtin__.print\n",
    "\n",
    "    def print(*args, **kwargs):\n",
    "        force = kwargs.pop('force', False)\n",
    "        if is_master or force:\n",
    "            builtin_print(*args, **kwargs)\n",
    "\n",
    "    __builtin__.print = print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be132356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda121",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
