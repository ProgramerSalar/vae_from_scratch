import math 
import warnings
import torch.nn as nn 
import torch 
from typing import Optional
import torch.nn.functional as F


def trunc_normal_(tensor, 
                  mean=0.,
                  std=1.,
                  a=-2.,
                  b=2.):
    
    """
        this function come from : https://github.com/huggingface/pytorch-image-models/blob/main/timm/layers/weight_init.py#L8
        if you learn more: https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf

        tensor (torch.Tensor) : an n-dimensional tensor ,
        mean (torch.float) : the mean of the normal distribution,
        std (torch.float) : the standard deviation of the normal distribution,
        a (torch.float) : the minimum cutoff value,
        b (torch.float): the maximum cutoff value

        `trunc_normal_` ensures that the initial weights remain within a reasonable range, which helps to keep the initial 
        actiation of neurons from becoming too large or too small, especially with activation function like sigmoid or tanh.
        This prmotes more stable and effective learning during training.
    """ 

    def normal_cdf(x):
        # Computes standard normal cumulative distribution function 
        return (1. + math.erf(x / math.sqrt(2.))) / 2.
    
    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn("mean is more than 2 std from [a, b]"
                      "The distribution of values may be incorrect.",
                      stacklevel=2)
        

    # values are generated by using a truncated uniform distribution and 
    # then using the inverse CDF for the normal distribution
    # get upper and lower cdf values 
    l = normal_cdf((a - mean) / std)
    u = normal_cdf((a - mean) / std)

    # uniformally fill tensor with values from [l, u], then translate to [2l-1, 2u-1]
    tensor.uniform_(2 * l -1, 2 * u - 1)

    # use inverse cdf transform for normal distribution to get truncated standard normal 
    tensor.erfinv_()

    # Transform to proper mean, std 
    tensor.mul_(std * math.sqrt(2.))
    tensor.add_(mean)

    # clamp to ensure it's in the proper range 
    tensor.clamp_(min=a, max=b)

    return tensor



ACT2CLS = {
    "swish": nn.SiLU,
    "silu": nn.SiLU,
    "mish": nn.Mish,
    "gelu": nn.GELU,
    "relu": nn.ReLU,
}


def get_activation(act_fn: str) -> nn.Module:
    """Helper function to get activation function from string.

    Args:
        act_fn (str): Name of activation function.

    Returns:
        nn.Module: Activation function.
    """

    act_fn = act_fn.lower()
    if act_fn in ACT2CLS:
        return ACT2CLS[act_fn]()
    else:
        raise ValueError(f"activation function {act_fn} not found in ACT2FN mapping {list(ACT2CLS.keys())}")


class AdaGroupNorm(nn.Module):
    r"""
    GroupNorm layer modified to incorporate timestep embeddings.

    Parameters:
        embedding_dim (`int`): The size of each embedding vector.
        num_embeddings (`int`): The size of the embeddings dictionary.
        num_groups (`int`): The number of groups to separate the channels into.
        act_fn (`str`, *optional*, defaults to `None`): The activation function to use.
        eps (`float`, *optional*, defaults to `1e-5`): The epsilon value to use for numerical stability.
    """

    def __init__(
        self, embedding_dim: int, out_dim: int, num_groups: int, act_fn: Optional[str] = None, eps: float = 1e-5
    ):
        super().__init__()
        self.num_groups = num_groups
        self.eps = eps

        if act_fn is None:
            self.act = None
        else:
            self.act = get_activation(act_fn)

        self.linear = nn.Linear(embedding_dim, out_dim * 2)

    def forward(self, x: torch.Tensor, emb: torch.Tensor) -> torch.Tensor:
        if self.act:
            emb = self.act(emb)
        emb = self.linear(emb)
        emb = emb[:, :, None, None]
        scale, shift = emb.chunk(2, dim=1)

        x = F.group_norm(x, self.num_groups, eps=self.eps)
        print(f"what is the scale : {scale.shape} and shift: {shift.shape} and data {x.shape}")
        x = x * (1 + scale) + shift
        return x
    


class SpatialNorm(nn.Module):
    """
    Spatially conditioned normalization as defined in https://arxiv.org/abs/2209.09002.

    Args:
        f_channels (`int`):
            The number of channels for input to group normalization layer, and output of the spatial norm layer.
        zq_channels (`int`):
            The number of channels for the quantized vector as described in the paper.
    """

    def __init__(
        self,
        f_channels: int,
        zq_channels: int,
    ):
        super().__init__()
        self.norm_layer = nn.GroupNorm(num_channels=f_channels, num_groups=32, eps=1e-6, affine=True)
        self.conv_y = nn.Conv2d(zq_channels, f_channels, kernel_size=1, stride=1, padding=0)
        self.conv_b = nn.Conv2d(zq_channels, f_channels, kernel_size=1, stride=1, padding=0)

    def forward(self, f: torch.Tensor, zq: torch.Tensor) -> torch.Tensor:
        f_size = f.shape[-2:]
        zq = F.interpolate(zq, size=f_size, mode="nearest")
        norm_f = self.norm_layer(f)
        new_f = norm_f * self.conv_y(zq) + self.conv_b(zq)
        return new_f